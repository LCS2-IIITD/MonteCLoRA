{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/anaconda3/envs/cooperative/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from typing import Any, Optional, Union\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.hooks import AlignDevicesHook\n",
    "from accelerate.utils import named_module_tensors, offload_state_dict\n",
    "from torch import nn\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "from torch.distributions.wishart import Wishart\n",
    "from torch.distributions.gamma import Gamma\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "eps = 1e-3\n",
    "class AsyncMonteCLoRASampler(threading.Thread):\n",
    "    def __init__(self, model, buffer_size=10, device='cpu'):\n",
    "        super().__init__(daemon=True)\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.buffer_size = buffer_size\n",
    "        self.queue = queue.Queue(maxsize=buffer_size)\n",
    "        self.running = True\n",
    "\n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            if self.queue.qsize() < self.buffer_size:\n",
    "                # print(self.queue.qsize())\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        z_mvn = torch.randn(\n",
    "                            (self.model.num_experts, self.model.in_features, self.model.out_features),\n",
    "                            device=self.device\n",
    "                        )\n",
    "                        wishart_sampler = Wishart(\n",
    "                            df=self.model.out_features,\n",
    "                            scale_tril=torch.eye(self.model.out_features, device=self.device)\n",
    "                        )\n",
    "                        z_wishart = wishart_sampler._bartlett_sampling(torch.Size())\n",
    "\n",
    "                        z_dirichlet = torch.randn(self.model.num_experts, device=self.device)\n",
    "\n",
    "                        sample = {\n",
    "                            'z_mvn': z_mvn,\n",
    "                            'z_wishart': z_wishart,\n",
    "                            'z_dirichlet': z_dirichlet\n",
    "                        }\n",
    "\n",
    "                        self.queue.put_nowait(sample)\n",
    "                except KeyboardInterrupt:\n",
    "                    sys.exit(0)\n",
    "                except Exception as e:\n",
    "                    print(f\"[AsyncMonteCLoRASampler] Error: {e}\")\n",
    "            else:\n",
    "                time.sleep(0.01)\n",
    "\n",
    "    def get(self):\n",
    "        try:\n",
    "            return self.queue.get_nowait()\n",
    "        except queue.Empty:\n",
    "            return None\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.join(timeout=1.0)\n",
    "\n",
    "class MonteCLoRASampler(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features, \n",
    "        out_features, \n",
    "        num_experts,\n",
    "        fan_in_fan_out=False,\n",
    "        use_entropy=True,\n",
    "        dirichlet_prior=1,\n",
    "        sample_scaler=3e-4,\n",
    "        kl_loss_weight=1e-5,\n",
    "        mc_training=True,\n",
    "        buffer_size=100,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_experts = num_experts\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        self.use_entropy = use_entropy\n",
    "        self.device = device\n",
    "        self.sample_scaler = sample_scaler\n",
    "        self.kl_loss_weight = kl_loss_weight\n",
    "        self.mc_training = mc_training\n",
    "        self.dirichlet_prior = dirichlet_prior\n",
    "\n",
    "        self.std_prior = nn.Parameter(torch.rand(out_features))\n",
    "        # self.expert_weights_prior = nn.Parameter(torch.rand(num_experts))\n",
    "        self.gaussian_var_prior = torch.eye(out_features).to(device)\n",
    "        self.expert_weights = torch.ones(num_experts, device=device) / num_experts\n",
    "\n",
    "        self.sampler = None\n",
    "        if self.mc_training:\n",
    "            self.sampler = AsyncMonteCLoRASampler(self, buffer_size, device)\n",
    "            self.sampler.start()\n",
    "\n",
    "    def wishart_reparameterization(self, std, z_wishart):\n",
    "\n",
    "        updated_var = std @ z_wishart @ std.T\n",
    "        updated_var = torch.diag(torch.clip(updated_var.diag(), min=eps))\n",
    "        return updated_var\n",
    "\n",
    "    def multivariate_reparameterization(self, z_mvn, cov_matrix):\n",
    "\n",
    "        L = torch.linalg.cholesky(cov_matrix).to(cov_matrix.dtype)\n",
    "        varsum = z_mvn @ L#torch.einsum('eio,op->eip', z_mvn, L) \n",
    "        varsum = torch.nan_to_num(varsum, nan=eps)\n",
    "        return self.sample_scaler * varsum\n",
    "\n",
    "    def dirichlet_reparameterization(self, alpha, z_dirichlet):\n",
    "        mu = torch.log(alpha) - torch.log(alpha).mean()\n",
    "        sigma = torch.diag(1/alpha * (1 - 2/self.num_experts) + 1/(self.num_experts**2) * (1/alpha).sum())\n",
    "        L = torch.linalg.cholesky(sigma)\n",
    "        return L @ z_dirichlet + mu\n",
    "\n",
    "    def calculate_entropy(self, expert_weights):\n",
    "        return (expert_weights ** 2).sum()\n",
    "\n",
    "    def dirichlet_kl(self, alpha2):\n",
    "        alpha1 = torch.tensor([self.dirichlet_prior]*self.num_experts, device=self.device)\n",
    "        gamma = lambda v: torch.lgamma(v).exp()\n",
    "        return torch.log(gamma(alpha2.sum())/gamma(alpha1.sum())) + \\\n",
    "               (torch.log(gamma(alpha2)/gamma(alpha1))).sum() + \\\n",
    "               ((alpha2 - alpha1)*(torch.digamma(alpha2) - torch.digamma(alpha2.sum()))).sum()\n",
    "\n",
    "    def wishart_kl(self, std):\n",
    "        var = std @ std.T\n",
    "        var = torch.diag(var.diag())\n",
    "        return 0.5 * (-torch.log(var).trace()*self.out_features + var.trace()*self.out_features - self.out_features**2)\n",
    "\n",
    "    def multivariate_kl(self, var):\n",
    "        var = torch.clamp(var, min=1e-6)\n",
    "        return self.num_experts * 0.5 * (var.trace() - torch.log(var).trace() - self.out_features)\n",
    "\n",
    "    def get_variational_loss(self):\n",
    "        if self.mc_training and self.training:\n",
    "            kl1 = 0 #self.dirichlet_kl(torch.exp(self.expert_weights_prior))\n",
    "            kl2 = self.wishart_kl(torch.diag(torch.exp(self.std_prior)))\n",
    "            kl3 = self.multivariate_kl(self.gaussian_var_prior)\n",
    "            entropy = self.calculate_entropy(self.expert_weights) if self.use_entropy else 0\n",
    "            return self.kl_loss_weight * (kl1 + kl2 + kl3), entropy\n",
    "        return 0, 0\n",
    "\n",
    "    def forward(self):\n",
    "        if self.training and self.mc_training:\n",
    "            # t = time.time()\n",
    "            sample = self.sampler.get() if self.sampler else None\n",
    "\n",
    "            if sample is not None:\n",
    "                z_mvn = sample['z_mvn']\n",
    "                z_wishart = sample['z_wishart']\n",
    "                # z_dirichlet = sample['z_dirichlet']\n",
    "            else:\n",
    "                z_mvn = torch.randn((self.num_experts, self.in_features, self.out_features), device=self.device)\n",
    "                wishart_sampler = Wishart(df=self.out_features, scale_tril=torch.eye(self.out_features, device=self.device))\n",
    "                z_wishart = wishart_sampler._bartlett_sampling(torch.Size())\n",
    "                # z_dirichlet = torch.randn(self.num_experts, device=self.device)\n",
    "            # temp = time.time() - t\n",
    "            # print(temp)\n",
    "\n",
    "            # t = time.time()\n",
    "            std = torch.diag(torch.exp(self.std_prior))\n",
    "            gaussian_var = self.wishart_reparameterization(std, z_wishart)\n",
    "            self.gaussian_var_prior = gaussian_var\n",
    "\n",
    "            var = self.multivariate_reparameterization(z_mvn, gaussian_var)\n",
    "\n",
    "            # expert_weights = self.dirichlet_reparameterization(torch.exp(self.expert_weights_prior), z_dirichlet)\n",
    "            # expert_weights = torch.ones(self.num_experts, device=self.device) / self.num_experts\n",
    "            # expert_weights = torch.sigmoid(expert_weights)\n",
    "            # expert_weights = expert_weights / expert_weights.sum()\n",
    "            # self.expert_weights = expert_weights\n",
    "            # temp2 = time.time()  - t\n",
    "            # print(temp2)\n",
    "            # print(temp2/temp)\n",
    "            return var, self.expert_weights\n",
    "        else:\n",
    "            return -1, -1\n",
    "\n",
    "    def eval(self):\n",
    "        if self.sampler:\n",
    "            self.sampler.stop()\n",
    "            self.sampler = None\n",
    "        super().eval()\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'sampler') and self.sampler:\n",
    "            self.sampler.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Assume sampler_list is defined and on the correct CUDA device\n",
    "device = torch.device(\"cuda:3\") # Or cuda:local_rank in DDP\n",
    "sampler_list = {f\"sampler_{i}\": MonteCLoRASampler(10,10,10, device=device) for i in range(10)}\n",
    "for sampler in sampler_list.values():\n",
    "    sampler.to(device)\n",
    "    sampler.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Streams took: 0.0433 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_samplers = len(sampler_list)\n",
    "    # Create a stream for each sampler call\n",
    "    streams = [torch.cuda.Stream(device=device) for _ in range(num_samplers)]\n",
    "\n",
    "    results = {}\n",
    "    outputs = [None] * num_samplers # Pre-allocate list for outputs\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Launch forward calls on different streams\n",
    "    sampler_items = list(sampler_list.items()) # Get (name, sampler) pairs\n",
    "    for i in range(num_samplers):\n",
    "        name, sampler = sampler_items[i]\n",
    "        stream = streams[i]\n",
    "        # --- IMPORTANT ---\n",
    "        # The forward call itself needs to happen within the stream context\n",
    "        # If forward() launches kernels, they go to 'stream'\n",
    "        with torch.cuda.stream(stream):\n",
    "             # Run the forward pass - operations inside will be queued on 'stream'\n",
    "             # We store the output directly, synchronization happens later\n",
    "             outputs[i] = sampler.forward() # Output is (var, weights) tuple\n",
    "\n",
    "    # Synchronize all streams to ensure computations are finished\n",
    "    # Option 1: Synchronize default stream with all others (simpler)\n",
    "    # torch.cuda.synchronize(device=device) # Waits for all kernels on the device\n",
    "\n",
    "    # Option 2: Synchronize each stream individually (more granular)\n",
    "    for stream in streams:\n",
    "        stream.synchronize()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"CUDA Streams took: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "    # Populate results dictionary after synchronization\n",
    "    for i in range(num_samplers):\n",
    "        name, _ = sampler_items[i]\n",
    "        results[name] = outputs[i]\n",
    "\n",
    "\n",
    "    # Example access\n",
    "    # var_0, weights_0 = results['sampler_0']\n",
    "    # if var_0 != -1: # Check for non-training output\n",
    "    #      print(f\"Sampler 0 var shape: {var_0.shape}\")\n",
    "    #      print(f\"Sampler 0 weights: {weights_0}\")\n",
    "\n",
    "else:\n",
    "    print(\"CUDA not available, skipping CUDA Streams example.\")\n",
    "    # Fallback to sequential execution or ThreadPoolExecutor if desired\n",
    "\n",
    "# --- Don't forget cleanup ---\n",
    "# for sampler in sampler_list.values():\n",
    "#      sampler.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "\n",
      "--- Testing CUDA Stream Path ---\n",
      "Stream Sampler: Train mode set.\n",
      "Stream Sampler: Simulating forward pass...\n",
      "Stream Sampler: Forward pass took: 0.0109 seconds\n",
      "Stream Sampler: Mean var sample shape: torch.Size([4, 768, 4])\n",
      "Stream Sampler: Mean expert weights shape: torch.Size([4])\n",
      "Stream Sampler: Calculating variational loss...\n",
      "Stream Sampler: Loss calculation took: 0.0038 seconds\n",
      "Stream Sampler: Combined Loss: 0.001938, Mean Entropy: 0.250000\n",
      "Stream Sampler: Loss requires grad.\n",
      "Stream Sampler: Setting to eval mode...\n",
      "Stream Sampler: Deleted.\n",
      "\n",
      "--- Testing ThreadPool Path ---\n",
      "Thread Sampler: Train mode set.\n",
      "Thread Sampler: Simulating forward pass...\n",
      "Thread Sampler: Forward pass took: 0.0265 seconds\n",
      "Thread Sampler: Mean var sample shape: torch.Size([4, 768, 4])\n",
      "Thread Sampler: Mean expert weights shape: torch.Size([4])\n",
      "Thread Sampler: Calculating variational loss...\n",
      "Thread Sampler: Loss calculation took: 0.0032 seconds\n",
      "Thread Sampler: Combined Loss: 0.002235, Mean Entropy: 0.250000\n",
      "Thread Sampler: Setting to eval mode...\n",
      "Thread Sampler: Deleted.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Keep all previous imports and the definitions of:\n",
    "# eps, AsyncMonteCLoRASampler, MonteCLoRASampler\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from typing import Any, Optional, Union, Dict, Tuple, List\n",
    "import sys\n",
    "import itertools\n",
    "import concurrent.futures\n",
    "\n",
    "import torch\n",
    "# Import cuda specifics if available\n",
    "if torch.cuda.is_available():\n",
    "    import torch.cuda\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.hooks import AlignDevicesHook\n",
    "from accelerate.utils import named_module_tensors, offload_state_dict\n",
    "from torch import nn\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "from torch.distributions.wishart import Wishart\n",
    "from torch.distributions.gamma import Gamma\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# Assume eps, AsyncMonteCLoRASampler, MonteCLoRASampler are defined as before\n",
    "# Paste their full definitions here if running stand-alone\n",
    "# ============================================================================\n",
    "eps = torch.finfo(torch.float32).eps\n",
    "\n",
    "# --- AsyncMonteCLoRASampler Definition ---\n",
    "class AsyncMonteCLoRASampler(threading.Thread):\n",
    "    def __init__(self, model, buffer_size=10, device='cpu'):\n",
    "        super().__init__(daemon=True)\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.buffer_size = buffer_size\n",
    "        self.queue = queue.Queue(maxsize=buffer_size)\n",
    "        self.running = True\n",
    "        self._fill_exception = None\n",
    "\n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            if self._fill_exception:\n",
    "                 time.sleep(0.1)\n",
    "                 continue\n",
    "            if self.queue.qsize() < self.buffer_size:\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        z_mvn = torch.randn(\n",
    "                            (self.model.num_experts, self.model.in_features, self.model.out_features),\n",
    "                            device=self.device\n",
    "                        )\n",
    "                        # Ensure df > p-1 for Wishart. Handle p=1 case.\n",
    "                        df_tensor = torch.tensor(float(max(self.model.out_features, 1)), device=self.device)\n",
    "                        if self.model.out_features == 0: # Avoid eye(0)\n",
    "                           z_wishart = torch.empty((0,0), device=self.device) # Or handle differently\n",
    "                        else:\n",
    "                           scale_tril = torch.eye(self.model.out_features, device=self.device)\n",
    "                           wishart_sampler = Wishart(df=df_tensor, scale_tril=scale_tril)\n",
    "                           z_wishart = wishart_sampler.sample() # Sample directly\n",
    "\n",
    "                        z_dirichlet = torch.randn(self.model.num_experts, device=self.device)\n",
    "\n",
    "                        sample = {\n",
    "                            'z_mvn': z_mvn,\n",
    "                            'z_wishart': z_wishart,\n",
    "                            'z_dirichlet': z_dirichlet\n",
    "                        }\n",
    "                        self.queue.put_nowait(sample)\n",
    "                except queue.Full:\n",
    "                     time.sleep(0.005)\n",
    "                except RuntimeError as e:\n",
    "                     print(f\"[AsyncMonteCLoRASampler] Runtime Error during sampling: {e}\")\n",
    "                     self._fill_exception = e\n",
    "                     time.sleep(0.1)\n",
    "                except Exception as e:\n",
    "                    if isinstance(e, KeyboardInterrupt):\n",
    "                        print(\"[AsyncMonteCLoRASampler] KeyboardInterrupt received, stopping.\")\n",
    "                        self.running = False\n",
    "                    else:\n",
    "                        print(f\"[AsyncMonteCLoRASampler] Error during sampling: {e}\")\n",
    "                        self._fill_exception = e\n",
    "                    time.sleep(0.1)\n",
    "            else:\n",
    "                time.sleep(0.01)\n",
    "\n",
    "    def get(self):\n",
    "        if self._fill_exception:\n",
    "            raise RuntimeError(f\"Async sampler encountered an error: {self._fill_exception}\") from self._fill_exception\n",
    "        try:\n",
    "            return self.queue.get_nowait()\n",
    "        except queue.Empty:\n",
    "            return None\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        while not self.queue.empty():\n",
    "            try:\n",
    "                self.queue.get_nowait()\n",
    "            except queue.Empty:\n",
    "                break\n",
    "        self.join(timeout=2.0) # Increased timeout slightly\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.is_alive():\n",
    "            self.stop()\n",
    "\n",
    "# --- MonteCLoRASampler Definition ---\n",
    "class MonteCLoRASampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        num_experts,\n",
    "        fan_in_fan_out=False,\n",
    "        use_entropy=True,\n",
    "        dirichlet_prior=1.0,\n",
    "        sample_scaler=3e-4,\n",
    "        kl_loss_weight=1e-5,\n",
    "        mc_training=True,\n",
    "        buffer_size=10,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_experts = num_experts\n",
    "        self.use_entropy = use_entropy\n",
    "        self.device = device\n",
    "        self.sample_scaler = sample_scaler\n",
    "        self.kl_loss_weight = kl_loss_weight\n",
    "        self.mc_training = mc_training\n",
    "        self.dirichlet_prior = float(dirichlet_prior)\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        self.log_std_prior = nn.Parameter(torch.randn(out_features, device=device))\n",
    "        self.register_buffer('last_gaussian_var', torch.eye(out_features, device=device), persistent=False)\n",
    "        self.register_buffer('last_expert_weights', torch.ones(num_experts, device=device) / num_experts, persistent=False)\n",
    "\n",
    "        self.sampler = None\n",
    "        self._start_sampler()\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def _start_sampler(self):\n",
    "        if self.mc_training and self.training and (self.sampler is None or not self.sampler.is_alive()):\n",
    "            self.sampler = AsyncMonteCLoRASampler(self, self.buffer_size, self.device)\n",
    "            self.sampler.start()\n",
    "\n",
    "    def _stop_sampler(self):\n",
    "        if hasattr(self, 'sampler') and self.sampler and self.sampler.is_alive():\n",
    "            self.sampler.stop()\n",
    "        self.sampler = None\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        # Clear potential error from previous runs when changing mode\n",
    "        if hasattr(self, 'sampler') and self.sampler:\n",
    "             self.sampler._fill_exception = None\n",
    "        if mode:\n",
    "            self._start_sampler()\n",
    "        else:\n",
    "            self._stop_sampler()\n",
    "        return super().train(mode)\n",
    "\n",
    "    def eval(self):\n",
    "        return self.train(False)\n",
    "\n",
    "    def wishart_reparameterization(self, log_std, z_wishart):\n",
    "        if self.out_features == 0:\n",
    "            return torch.empty((0,0), device=self.device)\n",
    "        std = torch.diag(torch.exp(torch.clamp(log_std, min=-10, max=10)))\n",
    "        updated_var = std @ z_wishart @ std\n",
    "        updated_var_diag = torch.diag(torch.clamp(updated_var.diag(), min=eps))\n",
    "        self.last_gaussian_var = updated_var_diag\n",
    "        return updated_var_diag\n",
    "\n",
    "    def multivariate_reparameterization(self, z_mvn, cov_matrix_diag):\n",
    "        if self.out_features == 0:\n",
    "            return torch.zeros_like(z_mvn) # Return zeros if no output features\n",
    "        std_dev = torch.sqrt(torch.diag(cov_matrix_diag)) # Get std dev vector\n",
    "        varsum = z_mvn * std_dev.unsqueeze(0).unsqueeze(1) # Broadcast std_dev\n",
    "        varsum = torch.nan_to_num(varsum, nan=eps)\n",
    "        return self.sample_scaler * varsum\n",
    "\n",
    "    def dirichlet_reparameterization(self, log_alpha, z_dirichlet):\n",
    "        # Placeholder: return fixed weights for now\n",
    "        fixed_weights = torch.ones(self.num_experts, device=self.device) / self.num_experts\n",
    "        self.last_expert_weights = fixed_weights\n",
    "        return fixed_weights\n",
    "\n",
    "    def calculate_entropy(self, expert_weights):\n",
    "        return (expert_weights ** 2).sum()\n",
    "\n",
    "    def dirichlet_kl(self, log_alpha2):\n",
    "        # Not currently used\n",
    "        return torch.tensor(0.0, device=self.device)\n",
    "\n",
    "    def wishart_kl(self, log_std):\n",
    "        if self.out_features == 0: return torch.tensor(0.0, device=self.device)\n",
    "        var = torch.exp(2 * log_std) # Variance vector = std^2\n",
    "        log_det_sigma = torch.sum(torch.log(var + eps))\n",
    "        trace_sigma = torch.sum(var)\n",
    "        p = self.out_features\n",
    "        kl_normal = 0.5 * (trace_sigma - log_det_sigma - p)\n",
    "        return kl_normal\n",
    "\n",
    "    def multivariate_kl(self, cov_matrix_diag):\n",
    "        if self.out_features == 0 or cov_matrix_diag.numel() == 0:\n",
    "             return torch.tensor(0.0, device=self.device)\n",
    "        var = torch.diag(cov_matrix_diag)\n",
    "        if var.numel() == 0: # Handle case where diag is empty\n",
    "             return torch.tensor(0.0, device=self.device)\n",
    "        log_det_sigma = torch.sum(torch.log(var + eps))\n",
    "        trace_sigma = torch.sum(var)\n",
    "        p = self.out_features\n",
    "        kl_normal = 0.5 * (trace_sigma - log_det_sigma - p)\n",
    "        return self.num_experts * kl_normal\n",
    "\n",
    "    def get_variational_loss(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if self.mc_training and self.training:\n",
    "            kl1 = torch.tensor(0.0, device=self.device) # dirichlet_kl\n",
    "            kl2 = self.wishart_kl(self.log_std_prior)\n",
    "            kl3 = self.multivariate_kl(self.last_gaussian_var)\n",
    "            total_kl = kl1 + kl2 + kl3\n",
    "            entropy = self.calculate_entropy(self.last_expert_weights) if self.use_entropy else 0.0\n",
    "            return self.kl_loss_weight * total_kl, entropy\n",
    "        else:\n",
    "            return torch.tensor(0.0, device=self.device), torch.tensor(0.0, device=self.device)\n",
    "\n",
    "    def forward(self) -> Tuple[Union[torch.Tensor, int], Union[torch.Tensor, int]]:\n",
    "        if self.training and self.mc_training:\n",
    "            if self.sampler is None or not self.sampler.is_alive():\n",
    "                 # Try to recover if sampler died unexpectedly\n",
    "                 print(f\"Warning: Sampler for {id(self)} not running in forward. Attempting restart.\")\n",
    "                 self._start_sampler()\n",
    "                 if self.sampler is None or not self.sampler.is_alive():\n",
    "                      print(f\"Error: Could not restart sampler for {id(self)}.\")\n",
    "                      # Decide fallback: error or synchronous? Let's return error indicator\n",
    "                      return -1, -1 # Indicate failure\n",
    "                 else:\n",
    "                    time.sleep(0.01) # Give thread time to start and maybe populate queue\n",
    "                    sample = self.sampler.get()\n",
    "            else:\n",
    "                try:\n",
    "                    sample = self.sampler.get()\n",
    "                except RuntimeError as e: # Catch error propagated from async sampler\n",
    "                     print(f\"Error retrieving sample from async sampler {id(self)}: {e}\")\n",
    "                     return -1, -1 # Indicate failure\n",
    "\n",
    "            if sample is None:\n",
    "                # print(\"Async queue empty, generating sample synchronously.\")\n",
    "                with torch.no_grad():\n",
    "                    z_mvn = torch.randn((self.num_experts, self.in_features, self.out_features), device=self.device)\n",
    "                    if self.out_features > 0:\n",
    "                        df_tensor = torch.tensor(float(self.out_features), device=self.device)\n",
    "                        scale_tril = torch.eye(self.out_features, device=self.device)\n",
    "                        wishart_sampler = Wishart(df=df_tensor, scale_tril=scale_tril)\n",
    "                        z_wishart = wishart_sampler.sample()\n",
    "                    else:\n",
    "                        z_wishart = torch.empty((0,0), device=self.device)\n",
    "            else:\n",
    "                z_mvn = sample['z_mvn']\n",
    "                z_wishart = sample['z_wishart']\n",
    "\n",
    "            gaussian_var_diag = self.wishart_reparameterization(self.log_std_prior, z_wishart)\n",
    "            var = self.multivariate_reparameterization(z_mvn, gaussian_var_diag)\n",
    "            # Use fixed expert weights for now\n",
    "            expert_weights = torch.ones(self.num_experts, device=self.device) / self.num_experts\n",
    "            self.last_expert_weights = expert_weights\n",
    "\n",
    "            return var, expert_weights\n",
    "        else:\n",
    "            return -1, -1\n",
    "\n",
    "    def __del__(self):\n",
    "        self._stop_sampler()\n",
    "\n",
    "# --- Updated MultiMonteCLoRASampler ---\n",
    "\n",
    "class MultiMonteCLoRASampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outer: int,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        num_experts: int,\n",
    "        fan_in_fan_out: bool = False,\n",
    "        use_entropy: bool = True,\n",
    "        dirichlet_prior: float = 1.0,\n",
    "        sample_scaler: float = 3e-4,\n",
    "        kl_loss_weight: float = 1e-5,\n",
    "        cosine_similarity_weight: float = 1e-5,\n",
    "        mc_training: bool = True,\n",
    "        buffer_size: int = 10,\n",
    "        device: str | torch.device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        use_cuda_streams: bool = True # New flag to control stream usage\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_outer = num_outer\n",
    "        self.mc_training = mc_training\n",
    "        self.cosine_similarity_weight = cosine_similarity_weight\n",
    "        self.kl_loss_weight = kl_loss_weight # Needed for individual samplers\n",
    "        self.use_cuda_streams = use_cuda_streams\n",
    "\n",
    "        if num_outer < 1:\n",
    "            raise ValueError(\"num_outer must be at least 1\")\n",
    "\n",
    "        # Ensure device is a torch.device object\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.samplers = nn.ModuleList()\n",
    "        for i in range(num_outer):\n",
    "            sampler = MonteCLoRASampler(\n",
    "                in_features=in_features,\n",
    "                out_features=out_features,\n",
    "                num_experts=num_experts,\n",
    "                fan_in_fan_out=fan_in_fan_out,\n",
    "                use_entropy=use_entropy,\n",
    "                dirichlet_prior=dirichlet_prior,\n",
    "                sample_scaler=sample_scaler,\n",
    "                kl_loss_weight=kl_loss_weight, # Pass down for internal calculation\n",
    "                mc_training=mc_training,\n",
    "                buffer_size=buffer_size,\n",
    "                device=self.device, # Use the unified device\n",
    "            )\n",
    "            self.samplers.append(sampler)\n",
    "\n",
    "        # Initialize CUDA streams if applicable and requested\n",
    "        self.streams = None\n",
    "        if self.device.type == 'cuda' and self.use_cuda_streams and torch.cuda.is_available():\n",
    "            # print(\"Initializing CUDA streams for MultiMonteCLoRASampler\") # Debug\n",
    "            self.streams = [torch.cuda.Stream(device=self.device) for _ in range(self.num_outer)]\n",
    "        elif self.device.type == 'cuda' and not torch.cuda.is_available():\n",
    "             print(\"Warning: Device specified as CUDA, but CUDA is not available. Falling back.\")\n",
    "             self.device = torch.device('cpu') # Fallback device\n",
    "\n",
    "        self.to(self.device) # Ensure wrapper module itself is on the correct device\n",
    "\n",
    "\n",
    "    def forward(self) -> Tuple[Union[torch.Tensor, int], Union[torch.Tensor, int]]:\n",
    "        \"\"\"\n",
    "        Calls the forward method of all internal samplers (concurrently if possible)\n",
    "        and returns the *mean* of their valid outputs.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Union[torch.Tensor, int], Union[torch.Tensor, int]]:\n",
    "            A tuple containing:\n",
    "                - mean_var (torch.Tensor): Mean of LoRA weight matrices delta_W across samplers.\n",
    "                                           Shape: (num_experts, in_features, out_features).\n",
    "                - mean_expert_weights (torch.Tensor): Mean of expert weights across samplers.\n",
    "                                                      Shape: (num_experts,).\n",
    "            Returns (-1, -1) if not in training mode, mc_training is False, or no\n",
    "            samplers produce valid output.\n",
    "        \"\"\"\n",
    "        if not self.training or not self.mc_training:\n",
    "            return -1, -1\n",
    "\n",
    "        valid_vars: List[torch.Tensor] = []\n",
    "        valid_weights: List[torch.Tensor] = []\n",
    "        outputs: List[Tuple[Union[torch.Tensor, int], Union[torch.Tensor, int]]] = [(None, None)] * self.num_outer\n",
    "\n",
    "        try:\n",
    "            # --- Execute forward passes concurrently ---\n",
    "            if self.streams: # Use CUDA Streams\n",
    "                # print(\"Using CUDA streams for forward pass...\") # Debug\n",
    "                for i, sampler in enumerate(self.samplers):\n",
    "                    with torch.cuda.stream(self.streams[i]):\n",
    "                        # Launch the forward pass onto the stream\n",
    "                        outputs[i] = sampler.forward()\n",
    "                        # Note: Execution is asynchronous here. Result tensors are not ready yet.\n",
    "\n",
    "                # Synchronize all streams to wait for completion\n",
    "                # torch.cuda.synchronize(self.device) # Waits for all kernels on device\n",
    "                # Alternative: synchronize streams individually (might be slightly more efficient if streams finish at different times)\n",
    "                for stream in self.streams:\n",
    "                     stream.synchronize()\n",
    "                # print(\"CUDA streams synchronized.\") # Debug\n",
    "\n",
    "                # Now collect results from the 'outputs' list\n",
    "                for i in range(self.num_outer):\n",
    "                    var, expert_weights = outputs[i]\n",
    "                    if isinstance(var, torch.Tensor) and isinstance(expert_weights, torch.Tensor):\n",
    "                        valid_vars.append(var)\n",
    "                        valid_weights.append(expert_weights)\n",
    "                    # else: print(f\"Sampler {i} returned invalid output: {outputs[i]}\") # Debug\n",
    "\n",
    "            else: # Use ThreadPoolExecutor (CPU or no streams requested)\n",
    "                # print(\"Using ThreadPoolExecutor for forward pass...\") # Debug\n",
    "                futures = {}\n",
    "                # Limit workers? Or use num_outer? Let's use num_outer for max potential parallelism.\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_outer) as executor:\n",
    "                    for i, sampler in enumerate(self.samplers):\n",
    "                        futures[i] = executor.submit(sampler.forward)\n",
    "\n",
    "                    for i in range(self.num_outer):\n",
    "                        try:\n",
    "                            var, expert_weights = futures[i].result()\n",
    "                            if isinstance(var, torch.Tensor) and isinstance(expert_weights, torch.Tensor):\n",
    "                                valid_vars.append(var)\n",
    "                                valid_weights.append(expert_weights)\n",
    "                            # else: print(f\"Sampler {i} returned invalid output: {(var, expert_weights)}\") # Debug\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error collecting result from sampler {i} thread: {e}\")\n",
    "                            # Decide how to handle thread errors, e.g., continue without its result\n",
    "\n",
    "        except RuntimeError as e:\n",
    "             # Catch potential CUDA errors during stream operations or synchronization\n",
    "             print(f\"Runtime Error during multi-sampler forward pass: {e}\")\n",
    "             return -1, -1 # Indicate failure\n",
    "        except Exception as e:\n",
    "             print(f\"Unexpected Error during multi-sampler forward pass: {e}\")\n",
    "             return -1, -1 # Indicate failure\n",
    "\n",
    "        # --- Average the valid results ---\n",
    "        if not valid_vars or not valid_weights:\n",
    "            # print(\"No valid outputs received from samplers.\") # Debug\n",
    "            return -1, -1 # Return error indicator if no sampler succeeded\n",
    "\n",
    "        # Stack tensors along a new dimension (dim=0) and compute the mean\n",
    "        # Ensure all tensors are on the correct device before stacking (should be already)\n",
    "        try:\n",
    "            mean_var = torch.stack(valid_vars).mean(dim=0)\n",
    "            mean_expert_weights = torch.stack(valid_weights).mean(dim=0)\n",
    "        except RuntimeError as e:\n",
    "             # Catch potential shape mismatches if samplers somehow returned different shapes\n",
    "             print(f\"Error averaging sampler outputs (check shapes?): {e}\")\n",
    "             # Optionally inspect shapes:\n",
    "             # for i, v in enumerate(valid_vars): print(f\"Var {i} shape: {v.shape}\")\n",
    "             # for i, w in enumerate(valid_weights): print(f\"Weights {i} shape: {w.shape}\")\n",
    "             return -1, -1\n",
    "\n",
    "        return mean_var, mean_expert_weights\n",
    "\n",
    "    def get_variational_loss(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculates the combined variational loss across all samplers.\n",
    "        (Loss logic remains the same as before - operates on individual sampler states)\n",
    "        \"\"\"\n",
    "        if not self.training or not self.mc_training:\n",
    "            return torch.tensor(0.0, device=self.device), torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        total_kl_loss = torch.tensor(0.0, device=self.device)\n",
    "        total_entropy = torch.tensor(0.0, device=self.device)\n",
    "        sampler_states = [] # Store states needed for cosine similarity\n",
    "\n",
    "        # --- 1. Aggregate KL and Entropy ---\n",
    "        for sampler in self.samplers:\n",
    "            try:\n",
    "                kl_loss, entropy = sampler.get_variational_loss()\n",
    "                total_kl_loss += kl_loss\n",
    "                total_entropy += entropy\n",
    "                # Store the necessary state for cosine similarity calculation\n",
    "                # We need the diagonal variance computed in the *last* forward pass\n",
    "                sampler_states.append(sampler.last_gaussian_var) # Store the diagonal matrix\n",
    "            except Exception as e:\n",
    "                 print(f\"Error getting loss from sampler {id(sampler)}: {e}\")\n",
    "                 # Skip this sampler for loss calculation? Or handle differently?\n",
    "                 # Let's skip for now to avoid crashing.\n",
    "                 continue\n",
    "\n",
    "        if len(sampler_states) == 0: # If all samplers failed in get_variational_loss\n",
    "             return torch.tensor(0.0, device=self.device), torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        mean_kl_loss = total_kl_loss / len(sampler_states)\n",
    "        mean_entropy = total_entropy / len(sampler_states)\n",
    "\n",
    "        # --- 2. Calculate Cosine Similarity Penalty ---\n",
    "        total_cosine_similarity = torch.tensor(0.0, device=self.device)\n",
    "        num_pairs = 0\n",
    "\n",
    "        if len(sampler_states) >= 2 and self.cosine_similarity_weight > 0:\n",
    "            # Extract the diagonal vectors\n",
    "            prior_diagonals = [state.diag() for state in sampler_states if state.numel() > 0] # Ensure not empty\n",
    "\n",
    "            # Check if we still have enough vectors after filtering empty ones\n",
    "            if len(prior_diagonals) >= 2:\n",
    "                for i, j in itertools.combinations(range(len(prior_diagonals)), 2):\n",
    "                    vec_i = prior_diagonals[i].float()\n",
    "                    vec_j = prior_diagonals[j].float()\n",
    "\n",
    "                    # Check for non-zero vectors before similarity (cosine_similarity handles zero vectors with eps)\n",
    "                    # if torch.count_nonzero(vec_i) > 0 and torch.count_nonzero(vec_j) > 0:\n",
    "                    similarity = F.cosine_similarity(vec_i, vec_j, dim=0, eps=eps)\n",
    "                    similarity_penalty = torch.clamp(similarity, min=0.0)\n",
    "                    total_cosine_similarity += similarity_penalty\n",
    "                    num_pairs += 1\n",
    "            # else: print(\"Not enough valid prior diagonals for cosine similarity.\") # Debug\n",
    "\n",
    "\n",
    "        mean_cosine_similarity = (total_cosine_similarity / num_pairs) if num_pairs > 0 else torch.tensor(0.0, device=self.device)\n",
    "        cosine_similarity_loss = self.cosine_similarity_weight * mean_cosine_similarity\n",
    "\n",
    "        # --- 3. Combine Losses ---\n",
    "        total_weighted_loss = mean_kl_loss + cosine_similarity_loss\n",
    "\n",
    "        return total_weighted_loss, mean_entropy\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        if not isinstance(mode, bool):\n",
    "            raise ValueError(\"training mode is expected to be boolean\")\n",
    "        super().train(mode)\n",
    "        for sampler in self.samplers:\n",
    "            sampler.train(mode) # Propagate mode to children\n",
    "        return self\n",
    "\n",
    "    def eval(self):\n",
    "        return self.train(False)\n",
    "\n",
    "    def __del__(self):\n",
    "        # print(\"Deleting MultiMonteCLoRASampler...\") # Debug\n",
    "        # Ensure streams are cleaned up? Streams are usually managed by context or device reset.\n",
    "        # Explicitly stopping sampler threads is important.\n",
    "        for i, sampler in enumerate(self.samplers):\n",
    "            try:\n",
    "                 sampler.eval() # Calls _stop_sampler internally\n",
    "            except Exception as e:\n",
    "                 # Mute errors during deletion as interpreter might be shutting down\n",
    "                 pass\n",
    "                 # print(f\"Muted error cleaning up sampler {i} in MultiMonteCLoRASampler.__del__: {e}\")\n",
    "\n",
    "\n",
    "# --- Example Usage (Updated) ---\n",
    "\n",
    "N_OUTER = 2\n",
    "IN_DIM = 768\n",
    "OUT_DIM = 4\n",
    "N_EXPERTS = 4\n",
    "USE_GPU = True # Set to False to test CPU path\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    DEVICE = 'cuda:1'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Test CUDA Stream Path ---\n",
    "\n",
    "print(\"\\n--- Testing CUDA Stream Path ---\")\n",
    "multi_sampler_stream = MultiMonteCLoRASampler(\n",
    "    num_outer=N_OUTER,\n",
    "    in_features=IN_DIM,\n",
    "    out_features=OUT_DIM,\n",
    "    num_experts=N_EXPERTS,\n",
    "    kl_loss_weight=1e-5,\n",
    "    cosine_similarity_weight=5e-6,\n",
    "    buffer_size=20,\n",
    "    device=DEVICE,\n",
    "    use_cuda_streams=False # Explicitly request streams\n",
    ")\n",
    "multi_sampler_stream.train()\n",
    "print(\"Stream Sampler: Train mode set.\")\n",
    "\n",
    "print(\"Stream Sampler: Simulating forward pass...\")\n",
    "start_time = time.time()\n",
    "mean_var, mean_weights = multi_sampler_stream.forward()\n",
    "end_time = time.time()\n",
    "print(f\"Stream Sampler: Forward pass took: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "if isinstance(mean_var, torch.Tensor):\n",
    "    print(f\"Stream Sampler: Mean var sample shape: {mean_var.shape}\")\n",
    "    print(f\"Stream Sampler: Mean expert weights shape: {mean_weights.shape}\")\n",
    "\n",
    "    print(\"Stream Sampler: Calculating variational loss...\")\n",
    "    start_time = time.time()\n",
    "    combined_loss, mean_entropy_term = multi_sampler_stream.get_variational_loss()\n",
    "    end_time = time.time()\n",
    "    print(f\"Stream Sampler: Loss calculation took: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Stream Sampler: Combined Loss: {combined_loss.item():.6f}, Mean Entropy: {mean_entropy_term.item():.6f}\")\n",
    "\n",
    "    if combined_loss.requires_grad:\n",
    "            print(\"Stream Sampler: Loss requires grad.\")\n",
    "    else:\n",
    "            print(\"Stream Sampler: Loss does not require grad.\")\n",
    "else:\n",
    "    print(\"Stream Sampler: Forward pass failed or returned non-tensor.\")\n",
    "\n",
    "print(\"Stream Sampler: Setting to eval mode...\")\n",
    "multi_sampler_stream.eval()\n",
    "# del multi_sampler_stream \n",
    "print(\"Stream Sampler: Deleted.\")\n",
    "time.sleep(0.5) \n",
    "\n",
    "print(f\"\\n--- Testing {'CPU' if DEVICE=='cpu' else 'ThreadPool'} Path ---\")\n",
    "multi_sampler_thread = MultiMonteCLoRASampler(\n",
    "    num_outer=N_OUTER,\n",
    "    in_features=IN_DIM,\n",
    "    out_features=OUT_DIM,\n",
    "    num_experts=N_EXPERTS,\n",
    "    kl_loss_weight=1e-5,\n",
    "    cosine_similarity_weight=5e-6,\n",
    "    buffer_size=20,\n",
    "    device=DEVICE,\n",
    "    use_cuda_streams=False\n",
    ")\n",
    "multi_sampler_thread.train()\n",
    "print(\"Thread Sampler: Train mode set.\")\n",
    "\n",
    "print(\"Thread Sampler: Simulating forward pass...\")\n",
    "start_time = time.time()\n",
    "mean_var_t, mean_weights_t = multi_sampler_thread.forward()\n",
    "end_time = time.time()\n",
    "print(f\"Thread Sampler: Forward pass took: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "if isinstance(mean_var_t, torch.Tensor):\n",
    "    print(f\"Thread Sampler: Mean var sample shape: {mean_var_t.shape}\")\n",
    "    print(f\"Thread Sampler: Mean expert weights shape: {mean_weights_t.shape}\")\n",
    "\n",
    "    print(\"Thread Sampler: Calculating variational loss...\")\n",
    "    start_time = time.time()\n",
    "    combined_loss_t, mean_entropy_term_t = multi_sampler_thread.get_variational_loss()\n",
    "    end_time = time.time()\n",
    "    print(f\"Thread Sampler: Loss calculation took: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Thread Sampler: Combined Loss: {combined_loss_t.item():.6f}, Mean Entropy: {mean_entropy_term_t.item():.6f}\")\n",
    "else:\n",
    "    print(\"Thread Sampler: Forward pass failed or returned non-tensor.\")\n",
    "\n",
    "print(\"Thread Sampler: Setting to eval mode...\")\n",
    "multi_sampler_thread.eval()\n",
    "# del multi_sampler_thread # Cleanup\n",
    "print(\"Thread Sampler: Deleted.\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread Sampler: Simulating forward pass...\n",
      "Thread Sampler: Forward pass took: 0.0120 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Thread Sampler: Simulating forward pass...\")\n",
    "multi_sampler_thread.train()\n",
    "start_time = time.time()\n",
    "mean_var_t, mean_weights_t = multi_sampler_thread.forward()\n",
    "end_time = time.time()\n",
    "print(f\"Thread Sampler: Forward pass took: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_var_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread Sampler: Mean var sample shape: torch.Size([4, 768, 4])\n",
      "Thread Sampler: Mean expert weights shape: torch.Size([4])\n",
      "Thread Sampler: Calculating variational loss...\n",
      "Thread Sampler: Loss calculation took: 0.0034 seconds\n",
      "Thread Sampler: Combined Loss: 0.000639, Mean Entropy: 0.250000\n"
     ]
    }
   ],
   "source": [
    "if isinstance(mean_var_t, torch.Tensor):\n",
    "    print(f\"Thread Sampler: Mean var sample shape: {mean_var_t.shape}\")\n",
    "    print(f\"Thread Sampler: Mean expert weights shape: {mean_weights_t.shape}\")\n",
    "\n",
    "    print(\"Thread Sampler: Calculating variational loss...\")\n",
    "    start_time = time.time()\n",
    "    combined_loss_t, mean_entropy_term_t = multi_sampler_thread.get_variational_loss()\n",
    "    end_time = time.time()\n",
    "    print(f\"Thread Sampler: Loss calculation took: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Thread Sampler: Combined Loss: {combined_loss_t.item():.6f}, Mean Entropy: {mean_entropy_term_t.item():.6f}\")\n",
    "else:\n",
    "    print(\"Thread Sampler: Forward pass failed or returned non-tensor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream Sampler: Train mode set.\n",
      "Stream Sampler: Simulating forward pass...\n",
      "Stream Sampler: Forward pass took: 0.0116 seconds\n",
      "Stream Sampler: Mean var sample shape: torch.Size([4, 768, 4])\n",
      "Stream Sampler: Mean expert weights shape: torch.Size([4])\n",
      "Stream Sampler: Calculating variational loss...\n",
      "Stream Sampler: Loss calculation took: 0.0046 seconds\n",
      "Stream Sampler: Combined Loss: 0.002058, Mean Entropy: 0.250000\n",
      "Stream Sampler: Loss requires grad.\n",
      "Stream Sampler: Setting to eval mode...\n",
      "Stream Sampler: Deleted.\n"
     ]
    }
   ],
   "source": [
    "multi_sampler_stream.train()\n",
    "print(\"Stream Sampler: Train mode set.\")\n",
    "\n",
    "print(\"Stream Sampler: Simulating forward pass...\")\n",
    "start_time = time.time()\n",
    "mean_var, mean_weights = multi_sampler_stream.forward()\n",
    "end_time = time.time()\n",
    "print(f\"Stream Sampler: Forward pass took: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "if isinstance(mean_var, torch.Tensor):\n",
    "    print(f\"Stream Sampler: Mean var sample shape: {mean_var.shape}\")\n",
    "    print(f\"Stream Sampler: Mean expert weights shape: {mean_weights.shape}\")\n",
    "\n",
    "    print(\"Stream Sampler: Calculating variational loss...\")\n",
    "    start_time = time.time()\n",
    "    combined_loss, mean_entropy_term = multi_sampler_stream.get_variational_loss()\n",
    "    end_time = time.time()\n",
    "    print(f\"Stream Sampler: Loss calculation took: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Stream Sampler: Combined Loss: {combined_loss.item():.6f}, Mean Entropy: {mean_entropy_term.item():.6f}\")\n",
    "\n",
    "    if combined_loss.requires_grad:\n",
    "            print(\"Stream Sampler: Loss requires grad.\")\n",
    "    else:\n",
    "            print(\"Stream Sampler: Loss does not require grad.\")\n",
    "else:\n",
    "    print(\"Stream Sampler: Forward pass failed or returned non-tensor.\")\n",
    "\n",
    "print(\"Stream Sampler: Setting to eval mode...\")\n",
    "multi_sampler_stream.eval()\n",
    "# del multi_sampler_stream \n",
    "print(\"Stream Sampler: Deleted.\")\n",
    "time.sleep(0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooperative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
